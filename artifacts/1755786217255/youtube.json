{
  "titles": [
    "RAG Agent: Build & Deploy in 25m | LangChain RAG Tutorial",
    "Step-by-Step RAG Agent Build for Beginners",
    "RAG Architecture Explained: Vector DB, Embeddings, Agent",
    "LangChain + LlamaIndex RAG Demo (Live Build)",
    "Reduce Hallucinations with a Retrieval Agent"
  ],
  "description": "Repo: https://github.com/your-username/rag-agent — Follow this step-by-step guide to build a Retrieval-Augmented Generation (RAG) agent from scratch. In this beginner-friendly demo we'll cover the core architecture, tooling choices, and a live build that indexes docs, creates embeddings, and wires up an LLM agent for accurate, source-grounded answers.\n\nYou'll get a clear architecture walkthrough (embeddings → vector DB → retriever → reranker → LLM agent), code snippets (LangChain / LlamaIndex), vector DB options (Pinecone, Weaviate, Milvus, Supabase), and deployment tips to reduce hallucinations, cost, and latency. Clone the repo, follow the timestamps below to jump to the section you need, and try the included demo dataset.\n\n00:00 Intro — Cold open & demo\n00:15 Quick before/after hallucination demo\n00:30 What is RAG? High-level architecture\n01:30 Setup & environment (keys, repo, sample docs)\n03:00 Ingesting documents & creating embeddings\n07:00 Indexing to a vector DB (Pinecone/Weaviate/Milvus/Supabase)\n12:00 Building the retriever & agent (LangChain / LlamaIndex)\n18:00 Testing retrieval quality & reducing hallucinations\n20:00 Deploying the agent (Docker / Vercel / Cloud Run)\n23:00 Optimization: k, chunk size, reranker, cost tuning\n24:30 Evaluation, monitoring & next steps",
  "tags": [
    "RAG agent",
    "retrieval-augmented generation",
    "LangChain",
    "LlamaIndex",
    "Pinecone",
    "Weaviate",
    "Milvus",
    "Supabase",
    "vector database",
    "embeddings",
    "vector search",
    "semantic search",
    "agent orchestration",
    "RAG tutorial",
    "RAG demo",
    "deploy RAG",
    "reduce hallucinations",
    "LLM agent",
    "production RAG",
    "RAG architecture",
    "open-source RAG",
    "embedding models",
    "indexing",
    "reranker",
    "in-context retrieval"
  ],
  "hashtags": [
    "#RAGAgent",
    "#RetrievalAugmentedGeneration",
    "#LangChain",
    "#LlamaIndex",
    "#Pinecone",
    "#Weaviate",
    "#Milvus",
    "#Supabase",
    "#Embeddings",
    "#VectorDB",
    "#SemanticSearch",
    "#AIDevelopment"
  ],
  "keywords": [
    "RAG agent",
    "retrieval augmented generation",
    "LangChain RAG tutorial",
    "LlamaIndex demo",
    "vector database tutorial",
    "Pinecone RAG",
    "Weaviate RAG",
    "Milvus tutorial",
    "Supabase vector search",
    "embedding models for RAG",
    "reduce hallucinations RAG",
    "deploy RAG agent",
    "RAG architecture",
    "open-source RAG",
    "RAG pipeline 2025",
    "LLM agent orchestration",
    "cost optimization LLM",
    "memory context window"
  ],
  "srt": "1\n00:00:00,000 --> 00:00:07,000\nWelcome — in this video I’ll build a production-ready RAG agent and deploy it live in under 25 minutes.\n\n2\n00:00:07,000 --> 00:00:14,000\nFirst, a quick demo: a vanilla LLM hallucinated on a question, and the RAG agent gave a source-backed correct answer.\n\n3\n00:00:14,000 --> 00:00:24,000\nGoal: explain architecture, choose tools, and show a hands-on LangChain / LlamaIndex build with a vector DB.\n\n4\n00:00:24,000 --> 00:00:34,000\nArchitecture highlights: embeddings -> vector database -> retriever -> optional reranker -> LLM agent orchestration.\n\n5\n00:00:34,000 --> 00:00:46,000\nTooling: we’ll demo Pinecone and Supabase for vectors, compare Weaviate and Milvus, and use OpenAI or local embedding models.\n\n6\n00:00:46,000 --> 00:01:00,000\nSetup: clone the repo, provision a vector DB, add API keys to env, and prepare a small demo dataset to index.\n\n7\n00:01:00,000 --> 00:01:14,000\nStep 1: ingest documents and create embeddings. Step 2: index to the vector DB. Step 3: wire the retriever to the agent.\n\n8\n00:01:14,000 --> 00:01:30,000\nNext up we'll walk through the code, show retrieval evaluation metrics, deploy a containerized endpoint, and optimize cost.",
  "thumbnail_prompt": "High-contrast, safe-for-work thumbnail: a bold headline \"Build in 25m\" in large sans-serif font on the left, with a split-screen showing a terminal window with highlighted code on the right and a clean architecture diagram (embeddings → Vector DB → Agent) overlayed. Include small recognizable logos for LangChain, LlamaIndex, and Pinecone near the bottom. Use vibrant purple and electric blue gradient background, white text with a subtle drop shadow, and an action shot of a developer's hand pointing at the terminal (no faces). Make it clear, modern, and tech-focused."
}